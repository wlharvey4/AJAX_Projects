# -*- mode:org; -*-

#+title:Thirty AJAX Projects
#+subtitle:{{{version}}} {{{date}}}
#+author:wlh4
#+date:2021-01-27 07:38
#+macro:version Version 0.0.2
#+macro:upload-date (eval (current-time-string))
#+bucket:pinecone-forest.com

{{{version}}} {{{date}}}

#+texinfo:@insertcopying


* Introduction to this Course
:PROPERTIES:
:unnumbered: t
:END:
Thirty  Fun mini  projects  to explore  using JavaScript  to  connect to  APIs,
retrieve JSON data, and use it within your web page.

- 107 lectures
- 21.5 lecture hours
- Intermediate level

** Course Description
One of the *MOST common questions* I get is using *AJAX* making requests to the
server and returning  *JSON data* outputting it  to a web Page. Here  it is THE
MOST asked for course  content with over 30 Projects to  build your skills with
*AJAX* and *JavaScript  DOM* manipulation. This is a unique  course loaded with
modern  JavaScript content  - coding  examples and  challenges in  almost every
lesson!!! Also includes a 100+ Page PDF AJAX JavaScript Guide.

Perfect to do over  30 days or faster. Fun unique projects  that will AMAZE you
and  help you  learn updating  skills  with AJAX.  Explore how  YOU can  create
Dynamic and Interactive Web content with data loaded from an API.

*Please  note  this  is  a  fast   paced  course  with  prerequisites*  ---  it
demonstrates how to use and apply JavaScript to connect with various endpoints,
make requests  and return JSON  formatted data demonstrated in  interactive fun
projects. *Prior  coding experience is  required* as lessons focus  on applying
necessary  JavaScript functions  to  create interactions  and dynamic  content.
There  will be  a focus  on DOM  based coding  and use  of JSON  and JavaScript
Objects.

*AJAX* allows you to communicate with  the server, exchange data and update the
page without having to refresh the page.

The two major features of AJAX allow you to do the following:

- Make requests to the server *without reloading the page*

- Receive and work with data from the server

Course is loaded with 30 Projects to  help you learn and practice connecting to
endpoints, returning  JSON data,  making fetch requests,  using JSON  data with
JavaScript  to create  Page  content, explore  GET POST  PUT  and DELETE  (CRUD
create, read,  update, and delete  are the  four basic functions  of persistent
storage) with JavaScript AJAX.

*Select, create manipulate page elements with Modern JavaScript code* --- /make
things  happen/.  *Create Interactive*  and  *fully  dynamic web*  applications
driven by  *JSON data*. Get user  selections, make the *AJAX  request* from the
user select and output to the web page using JavaScript Code.

Explore how  to make *fun  interactive projects*  to make AJAX  requests coming
from  page data  and output  the  content to  the page.  Dynamic Page  elements
generated  from AJAX  requests to  endpoints and  return JSON  data and  use it
within JavaScript to update and manipulate DOM page elements. Create Games, Get
JSON data  and let users  explore the  data. Dynamically generate  page content
with JavaScript.

*Connect to web endpoints* --- get JSON  data and output that content into your
web page with JavaScript.

*AJAX practice mini projects*  to help you learn more about  JSON data - making
fetch requests - debugging and more.

*Source Code is  included* --- mini projects are designed  to help practice and
learn more about AJAX  and how you can use it to create  content and update web
pages.

Take your web content  to the next level --- *Bring your  WEB Page to LIFE!!!!*
Allow your users to interact with—the new information via JavaScript seamlessly
displaying the updated content.

*Massive JavaScript  AJAX Course*  --- with  *30 customized  learning projects*
ready  to help  you learn  and expand  your portfolio.  If you  want to  create
DYNAMIC and INTERACTIVE web pages this is the course for YOU!!!

Create asynchronous web applications --- On the client side retrieves data from
the server  in the  background NO  PAGE reloads -  just modern  interactive web
content.

Ajax allows  web pages and web  applications to change the  content dynamically
without reloading the entire page.

*Modern JavaScript coding* using fetch for *GET POST PUT and DELETE* methods. See
it in  action, try  it out for  yourself. Ajax  is not a  new technology,  or a
different language, just existing technologies used  in new ways.It is a set of
web  development   techniques  working  together   to  create  a   smooth  user
experience - interacting with server JSON data.

This  course will  demonstrate  how  to incorporate  it  into  your projects  -
creating games, getting Google Sheet data, getting server JSON data via an API.
Loaded with testing apps  to get the most out of AJAX.  Taught by an instructor
with over 20 years of real world experience ready to help you learn and explore
AJAX - This  is a unique course  with content you will not  find anywhere else.
Join Today Code Now.

*Taught by an instructor with over 20 years of Web Development experience ready
to help you learn more about JavaScript.*

** The Thirty Course Projects

1. Create  a dynamic  list  JSON  LocalStorage List  -  project  #1 Create  an
  interactive List that gets JSON data from  a json file and generates a list.
  Using  JavaScript  create  page  elements,  add  event  listeners  for  user
  interactive content. Store  JSON data as a string to  local storage and load
  JSON list  as a JavaScript  Object on Page load.  Create a dynamic  list for
  users to add and remove list items, check list items off as completed.

2. Create AJAX  to Simple JSON file  and get JSON with JavaScript  - project #2
   Practice coding  creates AJAX requests to  local JSON files -  Create a JSON
   file valid JSON to  JavaScript Object. Load local JSON file  data and use it
   with JavaScript to  create page content. Add user event  listener to trigger
   AJAX request using JavaScript Fetch method.

3. AJAX to get JSON data from Wiki API - project #3 Using JavaScript Connect to
   an API endpoint  get Wiki Data and update JavaScript  create and update Page
   content  with  JSON data.  JSON  data  from  wikipedia  API -  using  modern
   javascript fetch make a request to  the wikipedia endpoint - return response
   JSON. Custom request parameters from user input field. Generate page content
   using JavaScript DOM.

4. JSON from WikiMap API examples with JavaScript Code - project #4 Explore how
   to connect  to WikiMap API  to get JSON data  to update and  manipulate page
   contents using JavaScript. Coding example  of practice project to learn more
   and explore more about connecting to APIs and how to use JavaScript Fetch to
   get JSON data and output to the web pages.

5. Multiple Endpoint  Loading JSON data Tester - project  #5 Explore connecting
   to various JSON endpoints - test  JSON data to Page content with JavaScript.
   Using interactive JavaScript  web pages to select and make  AJAX requests to
   various endpoints and outputting the JSON data as content to web pages.

6. Generate  Random User Data Output  to Page AJAX and  JS Random User API  5 -
   project #6 Create an interactive Dynamic application that connects to random
   user  API  generates  pages  content  depending  on  user  selection.  Fully
   interactive web application using AJAX  requests from user page selections -
   get  JSON data  and output  to yoru  web pages  with JavaScript  Code. Fetch
   request  methods -  use of  interactive coding  to API  and customizing  the
   request parameters and request URL from HTML input fields on the webpage.

7. Weather Data App with API JSON data - project #7 Connecting to a weather API
   to output data with user input values. Create custom query requests to get
   back JSON data to use on your webpage. AJAX to weather endpoint with page
   element creation using JavaScript.

8. Jokes  API get fun Jokes  for Web Page Content  - project #8 Connect  to the
   Chuck Norris  open Joke API -  get Jokes -  Get Category list -  return joke
   data  from  user selected  categories  and  more. User  interactive  Dynamic
   content with JavaScript AJAX requests.

9. Interactive App with Star Wars Data  - project #9 Create a fully interactive
   web  application that  dynamically loads  Star Wars  data from  an Open  API
   source. Return JSON data and output  the data object with JavaScript to your
   webpage.

10. Dynamic Data  driven Trivia  DataBase Game  - project  #10 Create  a fully
   functional Trivia game, track game progress, let the user select the number
   of questions, difficulty and category. Load  the data from a JSON endpoint.
   Output page contents from JSON data. Users can interact and make selections
   and JavaScript makes the AJAX requests to load requested data for the user.
   Game functions adding  gameplay with scoring and ability  to play unlimited
   rounds of Trivia Questions all dynamically loaded with AJAX.

11. Stackexchange API tester Request JSON - project #11 Massive API loaded with
    data  and request  URL -  perfect to  practice and  get more  familiar with
    complex JSON data - multiple routes  for request URLs and loading JSON data
    to web  pages with JavaScript  Coding. Explore how  you can connect  to the
    Stackexchange API and  Request JSON Data to test and  build interactive web
    applications that are  data driven and fully dynamic  with JavaScript Code.
    This API has  a lot of data  and great to familiarize  yourself with making
    AJAX requests  with fetch method  in JavaScript,  Getting JSON data  to use
    within JavaScript  Code, Create your  own JavaScript code examples  of AJAX
    requests and using JavaSCript to create interactive web page content.

12. Select Country Data from API -  project #12 Connect to a countries API that
    lists  over  250 countries  with  a  complex  JSON  object rich  with  data
    including objects and image paths and arrays. Chunk page data into multiple
    pages allowing the user to select and interact with JSON data content.

13. User Search  Country by  Name  get JSON  data  - project  #13 Dynamic  and
   interactive  page  content  with  JavaScript   -  loading  JSON  data  into
   JavaScript coding objects and outputting  and creating page element content
   with JavaScript DOM.

14. Introduction to JavaScript Quiz Game Project - project #14 Create a fully
    interactive quiz game using JavaScript with JSON data from server endpoint.
    Connect to an API - return back response data to generate the quiz game
    using the values from the data. Build interactive content track progress
    randomize question options. Use JavaScript DOM manipulation to select,
    create and update page elements. Check correct answers, score the player,
    build the gameplay loading dynamically from the JSON data. Load the content
    from a Google Sheet - create the full gameplay which can customize the
    questions and create the JavaScript game progress all using the JSON data.
    Output the game progress report to the player as a downloadable text file.u

15. Creating a  Joke Generating Web page  from JSON data -  project #15 Explore
    how to connect to  an api with limited requests per hour.  Build the app in
    less than 10  requests. Get JSON data - output  as JavaScript object create
    page content from data.

16. Project multiple  endpoints for JSON data user selected  content. - project
    #16 Using an array data of  items, connect to different endpoints, allowing
    the user to select the content they want to load. Dynamically create a user
    interaction list of urls and allow user to interact and output the selected
    content.

17. Project connecting to the Github API loading JSON data - project #17 Create
    a  dynamic interactive  web user  interface connecting  to Github  data and
    allowing the user to load selected content. Page content is interactive and
    content is driven from JSON data provided by the GitHub API.

18. Generate interactive Game with Dynamic  Wordlist - project #18 Loading game
    data dynamically  from a Google Sheet.  How to share Google  Sheet data for
    web applications. Create a word Scramble  game from scratch that loads word
    lists from shared  Google Sheet docs. Create  a word list -  load the sheet
    data  into your  JavaScript application  to generate  Scramble Words  Game.
    Loading game  data dynamically  from a  Google Sheet.  How to  share Google
    Sheet data  for web  applications. Complete  review of  JavaScript Scramble
    Game  with  full  dynamic  gameplay coming  from  a  dynamically  generated
    wordlist  which is  requested via  AJAX  and returned  as JSON  to the  web
    application.

19. Quiz Game  with JavaScript and AJAX -  project #19 Explore how to  set up a
    quiz game  which is  flexible and dynamically  generated content  from JSON
    data. Connect to  an API load JSON  data and output with  JavaScript to the
    DOM web page. Creating an interactive web game with dynamically loaded AJAX
    content. JavaScript Game  Project with data driven dynamically  from an API
    endpoint.  Using JavaScript  creates the  web application  making it  fully
    dynamic and  adjustable with  content. Explore  how to set  up a  quiz game
    which is flexible and dynamically generated content from JSON data. Connect
    to an API  load JSON data and  output with JavaScript to the  DOM web page.
    Creating an interactive web game with dynamically loaded AJAX content.

20. YouTube Search API  with JavaScript Connect to API -  project #20 Setup and
    connect  to the  YouTube  api to  search  and get  YouTube  Data into  your
    webpage. Dynamically create your webpage  from JSON content coming from the
    YouTube API. Create interactive clickable  content that loads from the user
    selection  data.  Use  JavaScript  to create  a  fully  functional  YouTube
    interactive web application.

21. Frontend API  Tester connecting  to Endpoints  - project  #21 How  to make
   JavaScript fetch requests and return JSON data from various endpoints. Test
   and  make interactive  content  that  is data  driven  from  JSON data  and
   dynamically   generate  the   page  content   with  JavaScript.   DOM  page
   manipulation with JavaScript Coding.

22. Get and POST Tester API JSON JavaScript - project #22 Using user input data
    select content from API. Send data to the server with AJAX using JavaScript
    fetch both  with GET and POST  methods. Handle response JSON  data from the
    server endpoint - output and generate response to the user. Get input field
    and form  data to practice sending  requests to testing endpoint  which can
    handle both  POST and GET  methods returning the  simulated data as  a JSON
    object.

23. Form  Submission with JavaScript  fetch JSON data  - project #23  Create an
    HTML form - submit  the form input contents with AJAX  to the server. Using
    formData create JSON from contents and  send to the endpoint. Simulate form
    submissions with  JavaScript and AJAX  using GET and POST  methods. Provide
    user detailed response and page content output from submission.

24. JSON placeholder  tester JSON  JavaScript -  project #24  JSON placeholder
   testing sending data to test server and retrieving response object as JSON.
   Output JSON response to the web  page with JavaScript. JavaScript DOM setup
   and manipulate  page elements outputs content  dynamically using JavaScript
   DOM element manipulation. Great mini  JavaScript and AJAX coding project to
   explore JSON and API sending fetch requests to a placeholder testing API.

25. Dynamic  Content interactive CRUD tester  Part 1 - project  #25 Select JSON
    data and  output to the  page. Create  user interaction options  to create,
    read,  update,  and delete  (CRUD)  server  side  data within  the  testing
    environment. Select item  by id value from server return  to page. List all
    available posts from server output to  page. Allow users to select items by
    id and update  the content, create JSON  objects to send to  the server and
    using PUT  method add to  server data. Create  new items for  the database.
    Delete post item by id with user selection. Update and customize output for
    user from  JSON data.  Using JavaScript DOM  manipulation update  and build
    dashboard of dynamic content coming from the testing Server data.

26. GitHub  json database  custom JSON  endpoint - project  #26 Connect  to the
    Massive GitHub API - create user search apply generating customized request
    URL with JavaScript Code. Use JavaScript  to make the AJAX request for JSON
    data. Output and  update page elements with Data from  the server. Show and
    display JSON  data values from  GitHub API. Explore connecting  and testing
    making  requests  to eh  server  output  contents  to  the web  pages  with
    JavaScript DOM coding.  Web page development and dynamic  web page content.
    Interactive and  user selected  JavaScript created  content driven  by JSON
    data from the API.

27. Setup  JSON Local Server  with Node  NPM - project  #27 Step by  step guide
    setup your own  local testing server. Practice JavaScript  AJAX requests to
    local data.  Create your own  JSON data -return  data locally from  json db
    file.  Perfect  for testing  and  exploring  making  requests to  your  own
    customized JSON data endpoint.

28. Connect  to Local  Server Get and  Post Requests with  Fetch -  project #28
    Practice connecting  to JSON data locally.  Setup your own JSON  data using
    JavaScript to connect to the local JSON  data and output it to your testing
    dev  app. Javascript  local web  developer environment  setup and  practice
    environment. Get JSON data use JavaScript to update page elements. Practice
    connecting and  making POST and GET  requests to local JSON  server using a
    json.db file  located locally. Create  page elements with  JavaScript using
    Data returned from JSON data object.

29. Connect  to Local  Server Put  and Delete Requests  with Fetch  project #29
    Practice connecting and making PUT and DELETE requests to local JSON server
    using a json.db file located  locally. Create page elements with JavaScript
    using Data  returned from  JSON data object.  Update Manipulate  JSON local
    data simulating Database requests to the backend code.

30. JavaScript  AJAX web contact form  sending Emails with Apps  Script project
    #30 How  to send an  email from client side  code AJAX Contact  form submit
    data  to Google  Sheet  and Send  emails from  Google  Apps Script  MailApp
    Service.  Create your  own custom  fully functional  web contact  form that
    sends  emails, tracks  form  content  into a  spreadsheet.  Setup your  own
    endpoint using Google Apps Script and  creating a fully functional web app.
    Send POST  request from  your web  page using AJAX  and JavaScript  fetch -
    create  JSON structure  of web  form data  from inputs.  Get web  page form
    fields, create JSON object.

** What You Will Learn
- How to connect to Web APIs make AJAX requests with JavaScript
- Applying JavaScript to create Data driven Web Content
- Create Interactive Web Content generated from JSON data
- JSON data and how to use JSON as JavaScript Objects within Code
- Creating Dynamic Data driven Web Applications with JavaScript
- Update parts of a web page, without reloading the whole page
- Create User-Friendly interactive web applications
- Boost the performance of your web pages
- How to create Responsive user interfaces
- Modern JavaScript Coding to apply and use JSON data within Web Pages

** Course Resources
*** Resource Guide
 - [[file:resources/pdf/Course+Guide+AJAX+JavaScript+.pdf][Course Guide AJAX JavaScript]]

*** NPM liveServer
- https://www.npmjs.com/package/live-server

*** JavaScript Arrays
- https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array

**** JavaScript Array Class
A *global  object* that  is used  in the  construction of  *arrays*, high-level
list-like objects.

***** Description
Arrays are:
- list-like objects
- whose prototype has methods to perform
- traversal and
- mutation operations.

Both size and elements are mutable. Because an array's length can change at any
time, and data can be stored at non-contiquous locations, JavaScript arrays are
not  guaranteed to  be dense.  If  you would  like such,  consider using  typed
arrays.

Arrays  must   use  integers   as  element   indexes.  JavaScript   arrays  are
zero-indexed.  Using an invalid index number returns =undefined=.

- Object property collection ::

  Setting  or  accessing  via  non-integers  using  bracket  notation  (or  dot
  notation) will set  or access a variable associated with  that array's *object
  property collection*.

  The array's  *object properties* and  list of *array elements*  are separate,
  and the array's traversal and mutation  operations cannot be applied to these
  named properties.

***** Array Operations
- creation :: =let fruits = ['Apple', 'Banana'];=
  - constructor :: =Array()=
  - static methods ::
    - ~Array.from~  :: Creates  a  new  Array instance  from  an array-like  or
      iterable object.
    - ~Array.isArray~  ::  Returns true if the  argument is an array,  or false
      otherwise.
    - ~Array.of~  ::   Creates a new Array  instance with a variable  number of
      arguments, regardless of number or type of the arguments.
- access :: =let first = fruits[0]=
- looping :: =forEach=
  #+begin_example
    fruits.forEach(function(item, index, array) {
      console.log(item, index)
  #+end_example
- *push* an element to end :: =let newLength = fruits.push('Orange')=
- *pop* an element from end :: =let last = fruits.pop()=
- *shift* an element from end ::
    =let first = fruits.shift() // remove Apple from the front=
- *unshift* an element onto beginning ::
    =let newLength = fruits.unshift('Strawberry') // add to the front=
- find an index in the array: ~indexOf~ ::
  : fruits.push('Mango')
  : let pos = fruits.indexOf('Banana') // => 1
- remove an item by index position ::
  : let removedItem = fruits.splice(pos, 1) // this is how to remove an item
  : => // ["Strawberry", "Mango"]
- Remove items from an index position ::
  : let removedItems = vegetables.splice(pos, n)
  : // this is how to remove items, n defines the number of items to be removed,
  : // starting at the index position specified by pos and progressing toward the end of array.
- Copy an Array ::
  : let shallowCopy = fruits.slice() // this is how to make a copy
  : => // ["Strawberry", "Mango"]
- Array Instance Properties ::
  - Array.prototype.length :: Reflects the number of elements in an array.
  - Array.prototype[@@unscopables]  :: A  symbol containing  property names  to
    exclude from a with binding scope.
- Array Instance Methods- ::
  - Array.prototype.at() ::
  - Array.prototype.concat() ::
  - Array.prototype.copyWithin() ::
  - Array.prototype.entries() ::
  - Array.prototype.every() ::
  - Array.prototype.fill() ::
  - Array.prototype.filter() ::
  - Array.prototype.find() ::
  - Array.prototype.findIndex() ::
  - Array.prototype.forEach() ::
  - Array.prototype.includes() ::
  - Array.prototype.indexOf() ::
  - Array.prototype.join() ::
  - Array.prototype.keys() ::
  - Array.prototype.lastIndexOf() ::
  - Array.prototype.map() ::
  - Array.prototype.pop() ::
  - Array.prototype.push() ::
  - Array.prototype.reduce() ::
  - Array.prototype.reduceRight() ::
  - Array.prototype.reverse() ::
  - Array.prototype.shift() ::
  - Array.prototype.slice() ::
  - Array.prototype.some() ::
  - Array.prototype.sort() ::
  - Array.prototype.splice() ::
  - Array.prototype.toLocaleString() ::
  - Array.prototype.toString() ::
  - Array.prototype.unshift() ::
  - Array.prototype.values() ::
  - Array.prototype[@@iterator]() ::

*** JSON Object and Methods
- https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON
- https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON/parse
- https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON/stringify

JSON  is  a  *syntax*  for /serializing/  objects,  arrays,  numbers,  strings,
booleans, and null. It is based upon JavaScript syntax but is distinct from it:
some JavaScript is not JSON.

The JSON object contains methods  for parsing JavaScript Object Notation (JSON)
and converting values to JSON.

**** Static Methods
***** JSON.parse
- ~JSON.parse(text[, revivor])~ ::

  Parse the string =TEXT= as JSON,  optionally transform the produced value and
  its properties,  and return  the value.  Any violations  of the  JSON syntax,
  including those  pertaining to the  differences between JavaScript  and JSON,
  cause a =SyntaxError= to be thrown.

  The =REVIVER= option allows for interpreting  what the =REPLACER= has used to
  stand in for other datatypes.

***** JSON.stringify
- ~JSON.stringify(value[, replacer[, space]])~

  Return  a JSON  string  corresponding to  the  specified =VALUE=,  optionally
  including  only  certain  properties  or   replacing  property  values  in  a
  user-defined manner.  By default, all  instances of =undefined=  are replaced
  with  =null=, and  other  unsupported  native data  types  are censored.

  The =REPLACER= option allows for specifying other behavior.

**** JSON Syntax
#+begin_src javascript
  JSON = null
      or true or false
      or JSONNumber
      or JSONString
      or JSONObject
      or JSONArray

  JSONNumber = - PositiveNumber
	    or PositiveNumber
  PositiveNumber = DecimalNumber
		or DecimalNumber . Digits
		or DecimalNumber . Digits ExponentPart
		or DecimalNumber ExponentPart
  DecimalNumber = 0
	       or OneToNine Digits
  ExponentPart = e Exponent
	      or E Exponent
  Exponent = Digits
	  or + Digits
	  or - Digits
  Digits = Digit
	or Digits Digit
  Digit = 0 through 9
  OneToNine = 1 through 9

  JSONString = ""
	    or " StringCharacters "
  StringCharacters = StringCharacter
		  or StringCharacters StringCharacter
  StringCharacter = any character
		    except " or \ or U+0000 through U+001F
		 or EscapeSequence
  EscapeSequence = \" or \/ or \\ or \b or \f or \n or \r or \t
		or \u HexDigit HexDigit HexDigit HexDigit
  HexDigit = 0 through 9
	  or A through F
	  or a through f

  JSONObject = { }
	    or { Members }
  Members = JSONString : JSON
	 or Members , JSONString : JSON

  JSONArray = [ ]
	   or [ ArrayElements ]
  ArrayElements = JSON
	       or ArrayElements , JSON
#+end_src
*** JSONLint
- https://jsonlint.com

*** JavaScript Fetch API
- https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API/Using_Fetch

*** Local Storage
- https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage

* Chapter

* Build Tools
:PROPERTIES:
:appendix: t
:custom_id: build-tools
:END:
** Makefile					:dependencies:env_vars:perl:
:PROPERTIES:
:appendix: t
:dependency1: make
:dependency2.0: AWS User account at https://aws.amazon.com
:dependency2.1: AWS cli v2 in PATH https://docs.aws.amazon.com/cli/index.html
:dependency2.2: See how to Install AWS CLI v2 at https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-mac.html
:dependency2.3: aws credentials: access token and secret access token stored in ~/.aws/credentials
:dependency2.4: AWS S3 buckets set up for serving a static web page
:dependency3: GitHub Account with personal access token stored in GITHUB_TOKEN
:dependency4: texinfo @6.7._
:dependency5: Emacs, Org-mode, Babel language 'shell' enabled
:env_var1: SYNC_ORG_TEMPLATE: holds the full path to this Template.org file
:env_var2: GITHUB_TOKEN: holds the GitHub personal access token
:env_var3: EDITOR: must hold a reference to a working emacsclient server
:env_var4: COLORS
:END:

#+pindex:Makefile
#+name:Makefile
#+header: :tangle Makefile
#+begin_src makefile

  ###############################################################################
  ### USER-DEPENDENT VARIABLES
  ### USE ENVIRONMENT VARIABLES WHENEVER POSSIBLE

  # NOTE: All environment variables need to be exported PRIOR to starting the
  # Emacs server as EDITOR in your shell startup files; otherwise, they will not
  # be available to Emacs.
  # When I moved from using Bash to Zsh, I inadvertently changed the order of
  # import, and started the Emacs server before importing, and caused a horrible
  # bug which caused the program to work on one computer but fail on another.

  # The absolute path to this Template file
  TEMPLATE := $(SYNC_ORG_TEMPLATE)


  ### TOOLS & RESOURCES
  # tools is a directory holding tangled scripts, such as cmprpl
  # resources is a directory holding static resources for the project
  # images is a directory holding jpg and png image files
  RESOURCES := resources
  TOOLS	    := $(RESOURCES)/tools
  IMAGES    := $(RESOURCES)/images
  CMPRPL    := $(TOOLS)/cmprpl

  # Use emacsclient as $EDITOR; make sure it is set in a shell startup file and
  # the server has been started.
  EMACS	  := $(EMACS)
  EDITOR  := $(EDITOR)

  # User’s personal GitHub token for authentication to GitHub
  # DO NOT HARD-CODE THIS VALUE
  GITHUB_TOKEN := $(GITHUB_TOKEN)

  # The AWS Command Line Interface (AWS CLI) is an open source tool
  # that enables you to interact with AWS services using commands in
  # your command-line shell.  It must be present on your system.  Run the 'make'
  # command 'install-aws-cli' to install it if you do not have it.  Be sure to
  # run 'aws configure' after installing it.  This will place your AWS
  # credentials into ~/.aws/credentials.
  AWS := aws
  S3  := $(AWS) s3
  CFD := $(AWS) cloudfront

  ### END OF USER-DEPENDENT VARIABLES
  ###############################################################################
  ### MAKE-GENERATED VARIABLES

  ### PROJ AND ORG
  # ORG is the name of this Org file with extension .org
  # PROJ is the project name---the Org file name without extension.

  ### NOTE: there can be only one Org file in the project directory;
  # so far this has not been a problem, but it might be.

  PWD  := $(shell pwd)
  ORG  := $(shell ls *.org)
  PROJ := $(basename $(ORG))

  ### NOTE: S is needed only for the Template file because of the way it is nested
  # one level deep in the Templates GitHub repo, which uses the plural form
  # of Templates, whereas this file uses the singular form, Template.  So when
  # the homepage link is updated, the curl command must be told to use the plural
  # form.	 This is obviously a hack only for my own use and can be removed once
  # I clean up this anomaly.

  ifeq ($(PROJ),$(basename $(notdir $(TEMPLATE))))
  S := s
  endif

  # The AWS S3 bucket to use to store the html source file; it is found at the
  # key #+bucket towards the beginning of the file and should include the appropriate
  # suffix (.com, .net, .org, etc)
  BUCKET       := $(shell $(EDITOR) --eval \
		 '(with-current-buffer (find-file-noselect "$(ORG)") \
		    (save-excursion \
		      (goto-char (point-min)) \
		      (re-search-forward "^\#[+]bucket:\\(.*\\)$$" nil t) \
		      (match-string-no-properties 1)))')
  S3_BUCKET    := s3://$(BUCKET)

  # Buckets set up to serve static web sites from S3 can use either http
  # or https protocols; some  http protocols will automatically redirect
  # to https;  however, some only use  http. I would like  to accomodate
  # both, and  so this code  finds the url's  that are in  my Cloudfront
  # account, which presumably will serve https.  If the url is not here,
  # then this must be set up to serve http instead.
  HTTP_S := $(shell $(CFD) list-distributions | perl -MJSON::PP -e \
	  '$$/=""; \
	   my @urls = (); \
	   my $$json=JSON::PP->new->decode(<STDIN>); \
	   for my $$item ( @{$$json->{"DistributionList"}{"Items"}} ) { \
		  push @urls, @{$$item->{"Aliases"}{"Items"}}; \
	   } \
	  my $$found = grep { /'$(BUCKET)'/ } @urls; \
	  print "http", ($$found ? "s" : "");')

  HTTPS_BUCKET := https://$(BUCKET)

  ### DIR, SRC
  # DIR is the .info name found at '#+texinfo_filename:<DIR>.info' (at
  # the bottom of this file in the export configuration settings)
  # without its extension, used as the INFO filename and the name of the
  # HTML export directory; this code uses the lowercased PROJ name if
  # there is no '#+texinfo_filename'.
  # SRC is HTML directory based upon the DIR name

  #DIR := $(shell $(EDITOR) --eval \
  #	'(with-current-buffer (find-file-noselect "$(ORG)") \
  #		(save-excursion \
  #		(goto-char (point-min)) \
  #		(re-search-forward "^\#[+]\\(?:texinfo_filename\\|TEXINFO_FILENAME\\):\\(.*\\).info$$" nil t) \
  #		(match-string-no-properties 1)))')

  DIR := $(shell sed -E -n "/^\#\+texinfo_filename/s/^.*:(.*)\.info$$/\1/p" $(ORG))
  ifeq ($(DIR),$(EMPTY))
	  DIR := $(shell echo $(PROJ) | tr "[:upper:]" "[:lower:]")
  endif

  SRC := $(DIR)/

  ### VERS: v1.2.34/
  # VERS is the version number of this Org document.
  # When sync is run after the version number has been updated, then VERS
  # picks up the newly-changed value.  VERS used to be staticly imbedded
  # when the Makefile was tangled, but it needs to be dynamic for
  # development.

  # QUERY: should this number be formatted like this, or should it be just the numbers?
  # The reason it includes them is the S3PROJ obtains the name from the S3 bucket, and
  # it includes them.  But it only includes them because I have made it so.  Not a good
  # reason just by itself.  The ending slash is not actually a part of the version, but
  # comes from the way the 'aws2 ls' command returns its values.	So VERS should probably
  # not include the trailing slash, although it doesn’t hurt anything.

  VERS := v$(shell $(EDITOR) --eval \
	  '(with-current-buffer (find-file-noselect "$(ORG)") \
		  (save-excursion \
		    (goto-char (point-min)) \
		    (re-search-forward "^\#[+]\\(?:macro\\|MACRO\\):version Version \\(\\(?:[[:digit:]]+[.]?\\)\\{3\\}\\)") \
		    (match-string-no-properties 1)))')/

  ### AWS
  # PROJ_LIST contains the list of projects currently uploaded to
  # the S3 bucket; each item contains the name of the project and its
  # current version.

  # Created function using elisp instead of the shell.
  # This variable contains an elisp list of strings of the form '("proj1-v1.2.3/" "proj2-v4.5.6/" ...)'
  # However, when it prints to the shell, the quotes are lost.
  # Need to make sure elisp's variable 'exec-path contains the proper $PATH instead of adding to 'exec-path.

  PROJ_LIST := $(shell $(EDITOR) --eval \
	  "(progn \
		  (require (quote seq)) (add-to-list (quote exec-path) (quote \"/usr/local/bin\")) \
		  (seq-map (lambda (s) (replace-regexp-in-string \"^\s+PRE \" \"\" s)) \
			  (seq-filter (lambda (s) (string-match-p (regexp-quote \" PRE \") s)) \
			  (process-lines \"$(AWS)\" \"s3\" \"ls\" \"$(S3_BUCKET)\"))))")

  ### S3PROJ
  # The name of the current project as obtained from S3: 'proj-v1.2.34/'
  # If there is no current project in the S3 bucket, then assign a value equal to
  # the Org project and version instead.  It is set to the project if found, and
  # NO if not found, then updated in the ifeq block below.
  S3PROJ := $(shell $(EDITOR) --eval \
		  '(let ((proj (seq-find (lambda (s) (string-match-p "$(DIR)" s)) (quote $(PROJ_LIST))))) \
		     (or proj (quote NO)))')

  ### PROJINS3
  # is used by make sync; this allows the index.html file to be generated the first
  # time the project is synced.  It is set to NO if this project is not currently in an
  # S3 bucket, and it is set to YES if it is.
  PROJINS3 :=

  ### S3VERS
  # The version of this project currently installed in the S3 bucket: 'v1.2.34/'
  # If there is no current version in the S3 bucket, then assign the version from
  # this Org file instead.
  S3VERS   :=

  # Update S3PROJ, S3VERS, and PROJINS3
  ifeq ($(S3PROJ), NO)
	  S3PROJ := $(DIR)-$(VERS)
	  S3VERS := $(VERS)
	  PROJINS3 := NO
  else
	  S3VERS := $(subst $(DIR)-,,$(S3PROJ))
	  PROJINS3 := YES
  endif

  ### GITHUB
  # USER is the current user's GitHub login name.

  # The user name used to be statically embedded into the Makefile
  # during tangle, but in an effort to make the Makefile dynamically
  # indepedent, dynamic code has replaced the static code.  The code
  # that placed the static name in the Makefile was a 'node' script that
  # ran in a separate Org process during tangle.	An unfortunate fact of
  # 'make' is that 'make' strips the quote marks from the string
  # obtained from the 'curl' command when the 'make shell' command
  # returns the string.	 This makes the string malformed JSON and
  # unparsable by most JSON parsers, including 'node’.	However,
  # 'perl'’s core module JSON::PP (but not JSON::XS) has facilities to
  # parse very malformed JSON strings.	Therefore, this dynamic code
  # uses 'perl' and the core module JSON::PP to parse the 'curl' string
  # into a 'perl' JSON object which can return the login name.	This
  # code should work with any version of 'perl' without having to
  # install any modules.

  USER	:= $(shell \
	    curl -sH "Authorization: token $(GITHUB_TOKEN)" https://api.github.com/user \
	    | \
	    perl -MJSON::PP -e \
		'$$/ = ""; \
		 my $$json = JSON::PP->new->loose->allow_barekey->decode(<STDIN>); \
		 print $$json->{login};' \
	    )
  SAVE		:= resources

  ### TEXINFO
  TEXI		:= $(PROJ).texi
  INFO		:= $(DIR).info
  INFOTN	:= $(shell $(EDITOR) --eval "(file-truename \"$(INFO)\")")
  PDF		:= $(PROJ).pdf
  INDEX		:= index.html
  HTML		:= $(DIR)/$(INDEX)
  DIR_OLD	:= $(DIR)-old

  ### AWS S3
  DST_OLD	:= $(S3_BUCKET)/$(S3PROJ)
  DST_NEW	:= $(S3_BUCKET)/$(DIR)-$(VERS)
  EXCL_INCL	:= --exclude "*" --include "*.html"
  INCL_IMAGES	:= --exclude "*" --include "*.jpg" --include "*.png"
  GRANTS	:= --grants read=uri=http://acs.amazonaws.com/groups/global/AllUsers
  S3SYNC	:= $(S3) sync --delete $(EXCL_INCL) $(SRC) $(DST_OLD) $(GRANTS)
  S3MOVE	:= $(S3) mv --recursive $(DST_OLD) $(DST_NEW) $(GRANTS)
  S3COPY	:= $(S3) cp $(INDEX) $(S3_BUCKET) $(GRANTS)
  S3REMOVE	:= $(S3) rm $(S3_BUCKET)/$(S3PROJ) --recursive
  S3IMAGESYNC	:= $(S3) sync $(INCL_IMAGES) $(IMAGES) $(S3_BUCKET)/$(IMAGES) $(GRANTS)

  ###############################################################################

  default: check texi info html pdf

  PHONY: default all check values boot \
	    texi info html pdf \
	    open-org open-texi open-html open-pdf \
	    clean dist-clean wiped-clean \
	    help sync update delete-proj \
	    install-aws-cli \
	    index-html upload-index-html

  values: check
	    @printf "$${BLUE}Values...$${CLEAR}\n"
	    @echo TEMPLATE:	$(TEMPLATE)
	    @echo EDITOR:	$(EDITOR)
	    @echo USER:		$(USER)
	    @echo PWD:		$(PWD)
	    @echo ORG:		$(ORG)
	    @echo TEXI:		$(TEXI)
	    @echo INFO:		$(INFO)
	    @ECHO INFOTN:	$(INFOTN)
	    @echo BUCKET:	$(BUCKET)
	    @echo PROJ:		$(PROJ) $S
	    @echo S3_BUCKET:	$(S3_BUCKET)
	    @echo HTTP_S:	$(HTTP_S)
	    @echo HTTPS_BUCKET:	$(HTTPS_BUCKET)
	    @echo VERS:		$(VERS)
	    @echo S3PROJ:	$(S3PROJ)
	    @echo S3VERS:	$(S3VERS)
	    @echo DIR:		$(DIR)
	    @echo DIR_OLD:	$(DIR_OLD)
	    @echo SRC:		$(SRC)
	    @echo DST_OLD:	$(DST_OLD)
	    @echo DST_NEW:	$(DST_NEW)
	    @echo PROJ_LIST:	"$(PROJ_LIST)"
	    @echo PROJINS3:	$(PROJINS3)

  check:
	    @printf "$${BLUE}Checking dependencies...$${CLEAR}\n"

	    @[[ -z $(BUCKET) ]] && \
	       { printf "$${RED}$(BUCKET) $${CYAN}must be set.$${CLEAR}\n"; exit 1; } || \
	       printf "$${CYAN}BUCKET: $${GREEN}$(BUCKET)$${CLEAR}\n";

	    @[[ -z $${GITHUB_TOKEN} ]] && \
	       { printf "$${RED}GITHUB_TOKEN $${CYAN}must be set.$${CLEAR}\n"; exit 1; } || \
	       printf "$${CYAN}GITHUB_TOKEN: $${GREEN}SET$${CLEAR}\n";

	    @[[ (-d ~/.aws) && (-f ~/.aws/credentials) && (-f ~/.aws/config) ]] && \
	       printf "$${CYAN}AWS credentials and config: $${GREEN}SET$${CLEAR}\n" || \
	       { printf "$${RED}~/.aws 'credentials' and 'config' must be set.$${CLEAR}\n"; exit 1; }

	    @[[ "$(shell $(EDITOR) --eval '(member (quote texinfo) org-export-backends)')" = "(texinfo)" ]] && \
		  printf "$${CYAN}Texinfo backend: $${GREEN}INSTALLED.$${CLEAR}\n" || \
		  { printf "$${YELLOW}Texinfo backend:$${CLEAR} $${RED}NOT INSTALLED; it must be installed.$${CLEAR}\n"; exit 1; }

	    @[[ $(shell $(EDITOR) --eval '(symbol-value org-confirm-babel-evaluate)') == "t" ]] && \
		  { printf "$${YELLOW}org-confirm-babel-evaluate:$${CLEAR} $${RED}T; set to NIL.$${CLEAR}\n"; exit 1; } || \
		  printf "$${CYAN}org-confirm-babel-evaluate: $${GREEN}OFF.$${CLEAR}\n\n"

  open-org: $(ORG)
	    @$(EDITOR) -n $(ORG)
  $(ORG):
	    @echo 'THERE IS NO $(ORG) FILE!!!'
	    exit 1

  texi: $(TEXI)
  $(TEXI): $(ORG)
	   @echo Making TEXI...
	   @$(EDITOR) -u --eval \
		  "(with-current-buffer (find-file-noselect \"$(ORG)\" t) \
			  (save-excursion \
			  (org-texinfo-export-to-texinfo)))"
	   @echo Done making TEXI.
  open-texi: texi
	   @$(EDITOR) -n $(TEXI)

  info: $(INFO)
  $(INFO): $(TEXI)
	   @echo Making INFO...
	   @makeinfo -o $(INFO) $(TEXI)
	   @$(EDITOR) -u -eval \
		  "(when (get-buffer \"$(INFO)\") \
			  (with-current-buffer (get-buffer \"$(INFO)\") \
				  (revert-buffer t t t)))"
	   @echo Done making INFO.

  open-info: info
	   @$(EDITOR) -u -eval \
		  "(if (get-buffer \"*info*\") \
			  (with-current-buffer (get-buffer \"*info*\") \
				(when (not (string= \"(symbol-value (quote Info-current-file))\" \"$(INFOTN)\")) \
					(info \"$(INFOTN)\")) \
				(revert-buffer t t t)) \
		      (info \"$(INFOTN)\"))"

  html: $(HTML)
  $(HTML): $(TEXI)
	   @echo Making HTML INFO..
	   @makeinfo --html -o $(DIR) $(TEXI)
	   @echo Done making HTML.
	   $(CMPRPL) $(DIR) $(DIR_OLD)
  open-html: html
	   @open $(HTML)

  # If pdftexi2dvi produces an error, it may still produce a viable PDF;
  # therefore, use --tidy.  If it produces an error, try to link the PDF;
  # if it does not produce an error, the PDF will be added to the top dir
  # and there will be no attempt to link.
  pdf:	$(PDF)
  $(PDF): $(TEXI)
	  @echo Making PDF INFO...
	  @-pdftexi2dvi --quiet --build=tidy $(TEXI) || ln -s $(PROJ).t2d/pdf/build/$(PDF) $(PDF)
	  @echo Done making PDF.
  open-pdf:pdf
	   @open $(PDF)

  sync:   $(HTML)
	  @echo Syncing version $(VERS) onto $(S3VERS)...
	  $(S3SYNC)
	  $(S3IMAGESYNC)
	  @echo Done syncing.
	  [[ $(VERS) != $(S3VERS) ]] && { echo Moving...; $(S3MOVE); echo Done moving.;  make homepage; } || :
	  [[ $(PROJINS3) = "NO" ]] && make homepage || :

  # This is a target-specific variable for updating the “description”
  # key on the GitHub repo page with the current version number.  It
  # first makes a curl call to the GitHub project repo, finds the
  # “description” line, pulls out the description only (leaving the old
  # version) and then prints the value with the current version number.
  # This value is used by the “homepage:” target in the PATCH call.
  # This method is arguably harder to code but faster to run than using
  # Perl with the JSON::PP module.

  homepage: description = $(shell \
	  curl -s \
		  -H "Authorization: token $(GITHUB_TOKEN)" \
		  https://api.github.com/repos/$(USER)/$(PROJ)$S | \
		  (perl -ne 'if (/^\s*\"description\":\s*\"(.*): v(?:(?:[[:digit:]]+[.]?){3})/) {print $$1}'))

  ### NOTE the use of the S variable at the end of PROJ; this is to handle
  # the singular case of the GitHub repo using the plural form, Templates
  # whereas the the Template.org file uses the singular form.
  homepage: $(ORG) upload-index-html
	    @echo Updating homepage...
	    @echo DESCRIPTION: $(description)
	    @echo VERS: $(VERS)
	    @curl -i \
		  -H "Authorization: token $(GITHUB_TOKEN)" \
		  -H "Content-Type: application/json" \
		  -X PATCH \
		  -d "{\"homepage\":\"$(HTTPS_BUCKET)/$(DIR)-$(VERS)\",\
		       \"description\":\"$(description): $(VERS)\"}" \
		  https://api.github.com/repos/$(USER)/$(PROJ)$S
	    @echo Done updating homepage.

  delete-proj:
	  @echo Deleting project $(PROJ)...
	  @curl -i \
		  -H "Authorization: token $(GITHUB_TOKEN)" \
		  -H "Accept: application/vnd.github.v3+json" \
		  -X DELETE \
		  https://api.github.com/repos/$(USER)/$(PROJ)$S
	  @$(S3REMOVE)
	  @make dist-clean
	  @make upload-index-html
	  @$(EDITOR) -u --eval "(kill-buffer \"$(ORG)\")"
	  @rm -rf "../$(PROJ)"
	  @echo Done deleting project.

  index-html: $(INDEX)
  $(INDEX): $(ORG)
	  @echo making index.html...
	  $(EDITOR) --eval \
	  "(with-current-buffer (find-file-noselect \"$(ORG)\") \
		  (save-excursion \
		    (org-link-search \"#project-index-title\") \
		    (org-export-to-file (quote html) \"index.html\" nil t)))"
	  @echo Done making index.html.

  upload-index-html: $(INDEX)
	   @echo Uploading index.html...
	   $(S3COPY)
	   @echo Done uploading index.html

  install-aws-cli:
	    curl "https://awscli.amazonaws.com/AWSCLIV2.pkg" -o "AWSCLIV2.pkg" && \
	    sudo installer -pkg AWSCLIV2.pkg -target / && \
	    which aws && aws --version
	    rm -rf AWSCLIV2.pkg

  clean:
	  @echo Cleaning...
	    -@rm *~ 2>/dev/null
	    -@for file in *.??*; \
	    do \
		    ext=$${file#$(PROJ).}; \
		    [[ ! $${ext} =~ org|texi|info|pdf|html ]] && rm -rv $${file}; \
	    done

  dist-clean: clean
	  @echo Dist Cleaning...
	    @${EDITOR} -u --eval \
	      "(kill-buffer \"$(ORG)\")"
	    -@rm -rf *.{texi*,info*,html*,pdf*} $(DIR) $(TOOLS)
	    -@for dir in *; \
		do \
		    [ -d $$dir -a $$dir != "$(DIR_OLD)" -a $$dir != $(SAVE) ] && \
		    rm -vr $$dir; \
		done

  wipe-clean: dist-clean
	  @echo Wipe Clean...
	    -@rm -rf Makefile Readme.md $(DIR_OLD)
	    @git checkout Makefile README.md

  git-ready: dist-clean
	    git checkout Makefile
	    git checkout README.md
	    git status

  help:
	    @echo '"make boot" tangles all of the files in Template'
	    @echo '"make default" makes the .texi file, the .info file, \
	    the html files, and the .pdf file.'
	    @echo

	    @echo '"make check" checks for prerequistes'
	    @echo '"make values" runs check and prints variable values'
	    @echo

	    @echo '"make texi" makes the .texi file'
	    @echo '"make info" makes the .info file'
	    @echo '"make html" makes the html distribution in a subdirectory'
	    @echo '"make pdf" makes the .pdf file'
	    @echo

	    @echo '"make open-org" opens the ORG program using emacsclient for editing'
	    @echo '"make open-texi" opens the .texi file using emacsclient for review'
	    @echo '"make open-html" opens the distribution index.html file \
	    in the default web browser'
	    @echo '"make open-pdf" opens the .pdf file'
	    @echo

	    @echo '"make sync" syncs the html files in the AWS S3 bucket BUCKET; \
	    you must have your AWS S3 bucket name in the env var AWS_S3_BUCKET; \
	    You must have your AWS credentials installed in ~/.aws/credentials'
	    @echo

	    @echo '"make install-aws-cli" installs the "aws cli v2" command-line tools'
	    @echo 'You also need to run "aws configure" and supply your Access Key and Secret Access Key'
	    @echo

	    @echo '"make clean" removes the .texi, .info, and backup files ("*~")'
	    @echo '"make dist-clean" cleans, removes the html distribution, \
	    and removes the build directory'
	    @echo '"make wipe-clean" wipes clean the directory, including old directories'
	    @echo

	    @echo '"make delete-proj" deletes the project from the file system, GitHub and AWS'

#+end_src

*** TODO Next
1. The CloudFront configuration needs to be updated recognize the new version
   directory that is created as part of the ~sync~ operation.

2. Update the GitHub HOME website link for each new sync operation.

3. Store on GitHub a version of each other format upon a sync operation (i.e.,
   the INFO and PDF versions)

** Compare Replace

#+begin_comment
The following source code tangles all files during an export operation. This is
to  make  sure  the  ~cmprpl~  source code  exists  in  the  ~resources/tools/~
directory before running  the Makefile target =html=. It also  makes sure there
is a Makefile on an initial export. The following code is not exported.
#+end_comment

#+name:tangle-org-file
#+header: :exports results :eval yes :results silent
#+begin_src emacs-lisp
(org-babel-tangle-file (buffer-file-name))
#+end_src

The  AWS ~sync~  command  relies  upon time  stamps  to  determine whether  two
programs are identical or not, as  well as content.  If two otherwise identical
files have  different time stamps,  ~sync~ will  assume they are  different and
will  process the  newer.   However, the  ~texinfo~  ~makeinfo --html~  command
produces all  new files even  if some files  (or most files)  remain unchanged.
This  means that  all files  will be  uploaded to  the AWS  S3 bucket  on every
iteration, even though the majority of the files are actually unchanged.

The ~cmprpl~  source code attempts to  resolve the issue of  identical exported
code having different  time stamps, thus defeating the benefit  provided by the
~aws2 s3 sync~ command uploading only changed files.

This program makes sure that a generated HTML directory exists: =$DIR_NEW=.  If
it doesn’t, then it is in an improper state and the program stops with an error
message.

The  program then  checks  if  an old  directory  exists,  =$DIR_OLD=.  If  one
doesn’t,  then one  is  created by  copying the  current  new directory.   This
provides a baseline  for comparisons going forward.  The program  exits at that
point. It is very important that  the =$DIR_OLD= directory not be deleted going
forward.

Given  that =$DIR_OLD=  exists, the  program then  loops through  all files  in
=$DIR_NEW= and  compares them  to the  files in =$DIR_OLD=.   If the  files are
identical, the =$DIR_OLD= file replaces the =$DIR_NEW= file while retaining the
old time stamp (using the ~-p~ option of ~cp~. If a file is different, then the
=$DIR_NEW= file  replaces the =$DIR_OLD=  file, thus giving it  updated content
and  an updated  time stamp.   If the  file does  not exist  in the  =$DIR_OLD=
directory, then it is added.

The  program then  loops through  all of  the files  in the  old directory  and
deletes  any that  do not  exist in  the new  directory.  Now  both directories
should be in sync.

#+caption:Compare Replace program
#+name:cmprpl
#+header: :mkdirp t
#+header: :shebang "#!/usr/bin/env bash"
#+begin_src sh :tangle resources/tools/cmprpl
  [[ $# -eq 2 ]] || { echo "ERROR: Incorrect command line arguments"; exit 1; }
  DIR_NEW=$1
  DIR_OLD=$2

  [[ -d $DIR_NEW ]] || { echo "ERROR: $DIR_NEW does not exist"; exit 1; }
  [[ -d $DIR_OLD ]] || { echo "CREATING: $DIR_OLD does not exist"; cp -a $DIR_NEW $DIR_OLD; exit 0; }

  for newfile in $DIR_NEW/*
  do
      oldfile=$DIR_OLD/$(basename $newfile)
      if [[ -e $oldfile ]]
      then
	 if cmp -s $newfile $oldfile
	 then
	     printf "${GREEN}copying OLD to NEW${CLEAR}: "
	     cp -vp $oldfile $newfile
	 else
	     printf "${PURPLE}copying NEW to OLD${CLEAR}: "
	     cp -vp $newfile $oldfile
	 fi
      else
	  printf "${BLUE}creating NEW in OLD${CLEAR}: "
	  cp -vp $newfile $oldfile
      fi
  done

  for oldfile in $DIR_OLD/*
  do
      newfile=$DIR_NEW/$(basename $oldfile)
      if [[ ! -e $newfile ]]
      then
	  printf "${RED}removing OLD${CLEAR}: "
	  rm -v $oldfile
      fi
  done
#+end_src


** Update Utility Commands
*** Get Parsed Org Tree
This function looks for an Org file in the present working directory, and if it
finds one returns  a parsed tree using  ~org-element-parse-buffer~.  It returns
=nil= if there is no Org file or if the found file is not in ~org-mode~.

#+name:get-parsed-org-tree
#+header: :results silent
#+begin_src emacs-lisp
(defun get-parsed-org-tree (&optional org-dir)
  "This function takes an optional directory name, changes to
that directory if given, otherwise uses the pwd, and finds an Org
file and returns its parsed tree, or nil if none found."
  (when org-dir
      (cd (file-name-as-directory org-dir)))
  (let ((buf (car-safe (find-file-noselect "*.org" nil nil t))))
    (if buf
	(with-current-buffer buf (org-element-parse-buffer))
      nil)))
#+end_src

*** Check for CID
This code  checks whether an  Org file contains  a =custom_id= of  a particular
value.  It accepts  a ~cid-value~ and an optional directory.   If the directory
is not given, then it defaults to the current directory.  If throws an error if
the directory does not exist.  It returns =nil= if the given directory does not
contain an Org file.   It returns =t= if the Org file  contains a node property
of   =custom_id=  and   value  ~cid-value~,   or   =nil=  if   not.   It   uses
~get-parsed-org-tree~.

#+name:org-tree-cid-p
#+header: :results silent
#+begin_src emacs-lisp
(defun org-tree-cid-p (cid-value &optional org-dir)
  "Check whether an org file contains a custom_id of CID"
  (let ((tree (get-parsed-org-tree org-dir)))
    (car (org-element-map tree 'property-drawer
	   (lambda (pd) (org-element-map (org-element-contents pd) 'node-property
			  (lambda (np)
			    (and
			     (string= "custom_id" (org-element-property :key np))
			     (string= cid-value (org-element-property :value np))))))
	   nil t))))
#+end_src

#+name:run-org-tree-cid-p
#+header: :var cid="build-tools"
#+header: :var dir="/usr/local/dev/programming/MasteringEmacs"
#+header: :var gpot=get-parsed-org-tree()
#+header: :var otcp=org-tree-cid-p()
#+header: :results value
#+header: :eval never-export
#+begin_src emacs-lisp
(org-tree-cid-p cid dir)
#+end_src

#+call: run-org-tree-cid-p(dir="/usr/local/dev/programming/MasteringEmacs")

** Bucket Index HTML
The bucket should contain a master ~index.html~  file that links to each of the
individual project  ~index.html~ files.  The  master ~index.html~ file  will be
placed at the root of  the bucket, ~https://<bucket-name>.com/~, and the bucket
must be set up to serve this ~index.html~ when the user hits the root.

*** Get Bucket Name
 This  code searches  for  the keyword-value  pair =bucket:<BUCKET-NAME>=  that
 should be  located towards the  beginning of the  file, and returns  the value
 =BUCKET-NAME= or nil if not found.

#+name: get-bucket-name
#+header: :results value
#+begin_src emacs-lisp
   (save-excursion
     (goto-char (point-min))
     (re-search-forward "^#\\+bucket:\\s*?\\(.*\\)$" nil t)
     (match-string-no-properties 1))
#+end_src

For some reason, ~get-bucket-name~ does not  work when called from the headline
[[#project-index-links][=Links for  bucket=]] below  when creating  =index.html=, even  if it  returns as
~(prin1 ...)~ and is  set up to ~:return output~; the  call receives =nil=. The
following code from ~bucket-name~, however, works. I don't know why.

#+name: bucket-name
#+header: :results output
#+header: :var bucket-name=get-bucket-name()
#+begin_src emacs-lisp
(prin1 bucket-name)
#+end_src

*** Bucket HTTPS URL
This  code calls  ~get-bucket-name~ and  returns the  value returned  as a  URL
string or nil.

#+name: bucket-https-url
#+header: :results value
#+header: :var b=get-bucket-name()
#+begin_src emacs-lisp
(concat "https://" b)
#+end_src

*** S3 Bucket URL
This code calls ~get-bucket-name~ and returns the AWS S3 bucket url.

#+name: s3-bucket-url
#+header: :results value
#+header: :var b=get-bucket-name()
#+begin_src emacs-lisp
(concat "s3://" b)
#+end_src

*** Bucket Projects List
This code uses the ~s3-bucket-url~ result to obtain the list of projects in the
bucket.  It does  this by calling the  AWS S3 high-level command  ~ls~ and then
removing the  =PRE= string in  each result.  The result  that is returned  is a
single  string that  can be  separated into  individual links  by breaking  the
string on spaces.

#+name: bucket-projects-list
#+header: :results output
#+header: :var bucket=s3-bucket-url()
#+begin_src sh
/usr/local/bin/aws s3 ls ${bucket} | sed -ne 's/^.*PRE //p'
#+end_src

*** Bucket Project Links
This code  uses the result  from ~bucket-projects-list~ to create  an unordered
list of  links written to  bucket projects, written  in Org-mode syntax.  It is
executed by a =#+call:= in [[*Bucket Index][*Bucket  Index]] during an HTML export of that subtree
to a file called =index.html=.

#+name: bucket-project-links
#+header: :var b-url=bucket-https-url()
#+header: :var projects=bucket-projects-list()
#+header: :results output raw
#+begin_src emacs-lisp
(seq-do (lambda (u) (princ (format "- [[%s/%sindex.html][~%s~]]
" b-url u u))) (split-string projects))
#+end_src

*** Bucket Index
    :PROPERTIES:
    :custom_id: project-index-title
    :export_file_name: index.html
    :export_subtitle: {{{version}}} created {{{upload-date}}}
    :END:
#+html_doctype: html5
#+options: toc:nil html5-fancy:t

#+html: <hr>

**** Links for bucket call_bucket-name()
     :PROPERTIES:
     :unnumbered: t
     :custom_id: project-index-links
     :END:

#+call: bucket-project-links()
** Project Readme
This adds the README.md template to a project. It should be customized uniquely
for the project.

#+name:project-readme
#+header: :tangle README.md
#+begin_src markdown
# TITLE
## Subtitle
## Author
## Date
## Version
# ABSTRACT
This is the Org Template file.	It is the parent of all other Org Info blogs,
and provides the source code for processing them in various different ways.
# INTRODUCTION
# CHAPTER
## Section
### Subsection
#+end_src

** Boot Template
:PROPERTIES:
:dependency1: EMACS:=:/Applications/MacPorts/Emacs.app/Contents/MacOS/Emacs or similar
:dependency2: EDITOR:=:emacsclient
:dependency3: =SYNC_ORG_TEMPLATE= defined as $DEV/Templates/Org/Template.org
:END:
Although running the command ~org-babel-tangle~ (=C-c C-v t=) from within Emacs
will install  everything, it would  be nice to have  a simple Makefile  that is
downloaded with this  file that could be  invoked to do the  same thing without
starting Emacs and Org-mode and keying in the ~org-babel-tangle~ command.  This
little Makefile should be stored on  GitHub along with the ~Template.org~ file.
When  the source  is extracted  to a  directory, then  running this  Makefile's
default rule  as simply ~make~  will extract the ~preprocess.el~  script, which
updates  =DEV= and  then  extracts the  full Makefile.   Because  this file  is
tangled along with the full Makefile, it simply gets tacked onto the end of the
big Makefile as an additional rule.   Now, running ~make~ runs the default rule
from the  main Makefile, which is  to extract everything, then  export to TEXI,
INFO, HTML, and PDF forms.

It is assumed that an Emacs server is running, and that the $EDITOR environment
variable is set to use ~emacsclient~.

#+name:boot-template
#+header: :tangle Makefile
#+begin_src makefile
  boot:
	  $(EDITOR) -u --eval \
		  "(with-current-buffer (car (find-file-noselect \"./*.org\" nil nil t)) \
			  (goto-char (point-min)) \
			  (re-search-forward \"^#[+]name:preprocess.el$$\") \
			  (org-babel-tangle (quote (4))) \
			  (save-buffer) \
			  (kill-buffer))" \
	  --eval \
		  "(let ((rsrcdir \"resources\") \
			 (subdirs (list \"tools\" \"images\"))) \
		     (mkdir rsrcdir t) \
		     (dolist (subdir subdirs) (mkdir (concat rsrcdir \"/\" subdir) t)))"
	  ./resources/tools/preprocess.el
#+end_src

** Preprocess Env Vars
The environment variable DEV can be  in different locations and will be spelled
differently based  on how the  local machine is set  up.  For instance,  on one
system,  it will  be at  ~$HOME/Dev~  while in  another  system it  will be  at
~/usr/local/dev~.  However, the =:tangle= keyword  does not expand variables in
the form ~${DEV}~,  but rather requires absolute  paths, like ~/usr/local/dev~.
Therefore, this program works like a preprocessor for environment variables set
up  as part  of  =:tangle= lines,  changing them  to  their system  environment
variable values prior to tangling.  It lives in the ~resources/tools~ directory.

#+name:preprocess.el
#+header: :mkdirp t
#+header: :tangle resources/tools/preprocess.el
#+header: :shebang "#!/opt/local/bin/emacs -Q --script"
#+begin_src emacs-lisp
  (with-current-buffer (car (find-file-noselect "./*.org" nil nil t))
    (save-excursion
    (goto-char (point-min))
    (let ((re-search-str "\\(?::tangle\\|load-file \\(?:[\\]*\\)?[\"]\\)\s*\\(.*?/[dD]ev\\)/")
          (dev (getenv "DEV")))
      (while
              (re-search-forward re-search-str nil t)
              (replace-match dev t nil nil 1)))
    (save-buffer)
    (require 'org)
    (org-babel-tangle)))
#+end_src

** Samples
#+begin_comment
(cd "~/Dev/Emacs/MasteringEmacs/")
"/Users/pine/Dev/Emacs/MasteringEmacs/"

(defun add-bucket (org bucket)
  "Add a bucket keyword BUCKET to the org file ORG."
  (interactive "fFile: \nsBUCKET: ")
  (with-current-buffer (find-file-noselect org)
    (let* ((tree (org-element-parse-buffer))
	   (ins (car (org-element-map tree (quote section)
		 (lambda (s)
		   (org-element-map s (quote keyword)
		     (lambda (kw) (when (equal "MACRO" (org-element-property :key kw)) (1- (org-element-property :end kw))))
		     nil nil :keyword))
		 nil t nil nil))))
      (goto-char ins)
      (insert (format "#+bucket:%s\n" bucket))
      ())))

(add-bucket "MasteringEmacs.org" "pinecone-forest")
nil

(defun hl-region (raw-hl)
  "Obtain the begin and end positions for a headline."
  (with-current-buffer (find-file-noselect (getenv "SYNC_ORG_TEMPLATE"))
    (let* ((tree (get-parsed-tree))
	   (hl (car-safe (org-element-map tree 'headline
			   (lambda (hl) (when
					    (string= raw-hl
						     (org-element-property :raw-value hl))
					  (org-element-context)))
			   nil nil t))))
      (cons
       (org-element-property :begin hl)
       (org-element-property :end hl))
      )))

(hl-region "Build Tools")

(4888 . 29646)

(defun get-hl-with-prop (org-dir hl-prop)
  "Given a directory containing an Org template file and a custom_id property name, return the headline containing that custom_id, or nil if none."
  (progn
    (cd org-dir)
    (let ((org-buf (car-safe (find-file-noselect "*.org" nil nil t))))
      (if org-buf
	  (with-current-buffer org-buf
	    (let ((tree (org-element-parse-buffer)))
	      (org-element-map tree 'headline
		(lambda (hl)
		  (let ((cid (org-element-property :CUSTOM_ID hl)))
		    (when (string= hl-prop cid)
		      (and
		       (message (format "Found the headline %s containing property %s." (org-element-property :raw-value hl) hl-prop))
		       hl))))
		nil t)))
	(and
	 (message (format "The directory %s does not contain an Org file." org-dir))
	 nil)))))

(get-hl-with-prop "~/Dev/Templates/Org" "build-tools")

(headline (:raw-value "Build Tools" :begin 4888 :end 29646 :pre-blank 0 :contents-begin 4902 :contents-end 29645 :level 1 :priority nil :tags nil :todo-keyword nil :todo-type nil :post-blank 1 :footnote-section-p nil :archivedp nil :commentedp nil :post-affiliated 4888 :FROM-FILE "Template" :CUSTOM_ID "build-tools" :APPENDIX "t" :title "Build Tools"))









;;; Add a keyword named 'bucket' just after the version macro.
;;; This function should be run from within the directory containing the Org file.
(defun add-bucket (org-file s3-bucket)
  "Add the name of the associated AWS S3 bucket to an Org templated file."
  (with-current-buffer (find-file-noselect org-file)
    (goto-char (point-min))
    (let* ((tree (org-element-parse-buffer))
	   ;; find the beginning position of the first headline to act as a limit
	   (hl1 (org-element-map tree (quote headline) (lambda (hl) (org-element-property :begin hl)) nil t)))
      ;; Check for the presence of a bucket keyword before the first headline
      (unless (re-search-forward "^#\\+bucket:" hl1 t)
	;; If no bucket keyword is found, search for a keyword MACRO with the value 'version'
	(org-element-map tree (quote keyword)
	  (lambda (kw) (when (and (string= "MACRO" (org-element-property :key kw))
				  (string-match-p "version" (org-element-property :value kw)))
			 ;; return the end position of the MACRO; subtract an empty line if there is one
			 (goto-char (- (org-element-property :end kw) (org-element-property :post-blank kw)))
			 (insert "#+bucket:" s3-bucket)
			 (newline)
			 (basic-save-buffer)
			 (message (format "Added bucket %s" s3-bucket))))
	  nil t)))))

(add-bucket "MasteringEmacs.org" "pinecone-forest.com")
nil

"Added bucket pinecone-forest.com"









(keyword (:key "MACRO" :value "version Version 0.0.108" :begin 148 :end 181 :post-blank 1 :post-affiliated 148 ...))
("TITLE" "SUBTITLE" "AUTHOR" "DATE" "MACRO" "TEXINFO" "TEXINFO" "CINDEX" "CINDEX" "CINDEX" "CINDEX" "CINDEX" ...)







((keyword (:key "MACRO" :value "version Version 0.0.107" :begin 148 :end 181 :post-blank 1 :post-affiliated 148 ...)))
#+end_comment

* List of Programs
:PROPERTIES:
:appendix: t
:END:
#+texinfo:@listoffloats Listing

* List of Examples
:PROPERTIES:
:appendix: t
:END:
#+texinfo:@listoffloats Example

* Copying
:PROPERTIES:
:copying:  t
:END:

Copyright \copy 2020 by {{{author}}}

* Concept Index
:PROPERTIES:
:index: cp
:appendix: yes
:END:

* Program Index
:PROPERTIES:
:index: pg
:appendix: yes
:END:

* Function Index
:PROPERTIES:
:index: fn
:appendix: yes
:END:

* Variable Index
:PROPERTIES:
:index: vr
:appendix: yes
:END:


* Configuration							   :noexport:
#+startup:content

#+todo: SOMEDAY(s@) TODO(t@) INPROGRESS(i@) WAIT(w@) | CANCEL(c@) DONE(d!)

#+options: H:4

#+texinfo_class: info
#+texinfo_header:
#+texinfo_post_header:
#+texinfo_dir_category:<DIR CATEGORY>
#+texinfo_dir_title:<DIR TITLE>
#+texinfo_dir_desc:<DIR DESCRIPTION>
#+texinfo_printed_title:AJAX_Projects---Thirty AJAX Projects


* Footnotes

[fn:1]In the browser, add =index.text= to the end of the URL to see the source.

[fn:2]Markdown requires the standard Perl library module Digest::MD5.


* Local Variables						   :noexport:
# Local Variables:
# fill-column: 79
# indent-tabs-mode: t
# eval: (auto-fill-mode)
# time-stamp-pattern: "8/^\\#\\+date:%:y-%02m-%02d %02H:%02M$"
# End:
